---
title: "UB01"
author: "Anatol"
date: "3/19/2022"
output:
  html_document:
    code_folding: hide
    df_print: kable
    fig_caption: yes
    fig_height: 5
    fig_width: 5
    theme: united
    self_contained: yes
    toc: yes
    toc_float: yes
    toc_level: 3
---

```{r, include = FALSE}
library(dplyr)
library(magrittr)
library(flair)
library(pipeR)
library(knitr)
library(rmarkdown)
library(shiny)
library(shinyWidgets)
library(arm)
library(rAmCharts)
library(xfun)
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, 
      x)
  } else x
}

xfun::pkg_load2(c("htmltools", "mime"))
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{css, echo=FALSE}
.description {
  color: gray;
  font-style: italics;
```
# Aufgabe 1: Einfache Regression

```{r include=FALSE}
library(car)
```

Deskription des Prestige-Datensatzes:

```{r}
summary(Prestige)
str(Prestige)
head(Prestige)
```

Bei der Variable type gibt es 4 NA's, die wir weglassen:

```{r}
data <- na.omit(Prestige)
str(data)
```

## 1a)

*Analysieren Sie das Einkommen (income) in den verschiedenen Berufstypen (type). Unterscheiden sich die einzelnen Berufstypen hinsichtlich ihres Einkommens? Fallt Ihnen eine speziﬁsche Problematik auf, welche viele Datensatze in der Praxis haben?*

```{r}
plot(data)
```

Die Plots sind hisichtlich der Hauptachse spiegelverkehrt.   
*prestige gegen women*: man kann keinen Zusammenhang im Scatterplot beobachten.  
*prestige gegen income*: hier lässt sich ein positiver Zusammenhang beobachten: mehr Income, mehr Prestge. Der Zusammenhang ist aber nicht linear.  
*prestige gegen education*: **klarer positver linearer Zusammenhang: mit mehr Bildung geht mehr Prestige einher.**  
*prestige gegen census*: schwer zu interpretieren.   
*prestige gegen type*: es gibt ein Zusammenhang zwischen Jobtyp und Prestige. 

```{r}
plot(data$education, data$income)
```

Es scheint ein positiver linearer Zusammenhang zwischen Bildung und Einkommen zu geben.  

Boxplots

```{r}
boxplot(data)
boxplot(data$income, main = colnames(data)[2])
```

Berufsgruppen mit NA

```{r}
rownames(Prestige)[is.na(Prestige$type)]
```

athletes, newsboys, babysitters and farmers konnten keiner Gruppe zugeordnet werden.  
Durchschnittliches Einkommen in verschiedenen Job-Typen:  
```{r}
tapply(data$income, data$type, mean)

```



```{r}
tapply(data$income, data$type, median)
tapply(data$income, data$type, min)
tapply(data$income, data$type, max)
tapply(data$income, data$type, sd)
```

Metrische Variablen
```{r}
hist(data$education, breaks = 5)
```

Density
```{r}
plot(density(data$education), main = "Dichteplot Bidlung", col = "red")
```


```{r}
hist(data$income, prob = TRUE, ylim = c(0, 0.00015), breaks = 17)
lines(density(data$income), col = "red")
```


```{r}
table(data$type)
barplot(table(data$type))
```

Einzelne Berufstypen unterscheiden sich hinsichtlich ihres Einkommens: wahrend blue-collar und white-collar Arbeiter ungefahr gleich verdienen, haben professionals deutlich hoheres Einkommen.  

Viele Datensatze haben in der Praxis die Problematik der fehlenden Werte in Beobachtungen (missing values, NA).

## 1b)
*Schatzen Sie, basierend auf a), ein geeignetes lineares Regressionsmodell. Beschranken Sie sich hierbei auf eine
erklarende Variable. Stellen Sie ihr Modell graﬁsch mit einer roten Regressionsgeraden dar. Nutzen Sie die
Funktion lm um ein lm-Objekt zu erstellen. Die Funktion abline nimmt auch solche Objekte als Argument
an.*
```{r}
plot(data[, 1:4])
```
 
Es scheint starker positiver linearer Zusammenhang zwischen prestige und education zu geben.

Einfaches lineares Regressionsmodell
```{r}
lin_reg <- lm(prestige ~ education, data = data)
summary(lin_reg)

```
```{r}
plot(prestige ~ education, data = data, main = "Einfaches lineares Modell")
abline(lin_reg, col = "red")
```


## 1c)
*Berechnen Sie die Parameterschatzer $β_0$ und $β_1$ mit der Kleinste-Quadrate-Methode.*

```{r}
y <- data$prestige
x <- data$education
n <- nrow(data)

y_mean <- mean(y)
x_mean <- mean(x)

beta1 <- (sum(x*y) - n*x_mean*y_mean)/(sum(x^2) - n*x_mean^2)

beta1
lin_reg$coefficients[2]
cov(x, y)/var(x)
```
```{r}
beta0 <- (sum(x*y) - beta1*sum(x^2))/sum(x)

beta0
lin_reg$coefficients[1]
y_mean - beta1*x_mean
```
## 1d)
*Berechnen Sie die total sum of squares (SST), die explained sum of squares (SSE) und die residual sum of
squares (SSR).*

```{r}
y_pred <- predict(lin_reg, data.frame(education=x))

sst <- sum((y - y_mean)^2)
sse <- sum((y_pred - y_mean)^2)
ssr <- sum((y - y_pred)^2)

sst
sse
ssr

sum((lin_reg$fitted - y_mean)^2)
sum((y - lin_reg$fitted)^2)


```

## 1e)
*Berechnen Sie das $R^2$ Ihres Modells.*
```{r}
sse/sst
1 - ssr/sst
summary(lin_reg)$r.squared
```


## 1f)
*Was passiert mit den Parameterschatzern wenn Sie die abhangige Variable mit 2 multiplizieren? Wie verhalt sich in diesem Fall das $R^2$ des Modells?*

```{r}
lin_reg_mul <- lm(y*2 ~ x)
summary(lin_reg_mul)


```

Wenn die abhangige Variable mit 2 multipliziert wird, verdoppeln sich $β_0$ und $β_1$. Standardfehler verdoppeln sich, t- und p-Werte bleiben gleich. $R^2$ verandert sich nicht.

# Aufgabe 2: Modellannahmen

Annahmen 1) $E(ε_i|x_i) = 0$ und 2) $Var(ε_i|x_i) = σ_ε^2$ konnen mit Hilfe von Tukey-Anscombe-Plots gepruft werden: Residuen (Ordinate) geplottet gegen die geschatzten Werte der endogenen Variablen (Abszisse).

```{r}
plot(lin_reg$residuals ~ lin_reg$fitted.values, xlab = "fitted", ylab = "residuals")
abline(h = 0, col = "red")
```
Die Residuen scheinen im Mittelwert 0 zu sein und eine konstante Variation aufzuweisen; die Annahmen 1) und 2) sind somit erfullt.  

Annahme 3) $Cov(ε_i, ε_j|x_i) = 0 ∀ i ≠ j$: Die Residuen werden gegen die Zeit (bzw. den Lauﬁndex der Beobachtungen) geplottet.

```{r}
plot(lin_reg$residuals, ylab = "residuals")
```

Es lasst sich keine Systematik in den Residuals beobachten; Annahme 3 ist auch erfullt.

Annahme 4 $ε_i|x_i ∼ N(0, σ_ε^2)$ wird durch einen Quantil-Quantil-Plot uberpruft (Q-Q-Plot). Hierbei werden die Quantile der Residuen gegen
die theoretischen N(0, 1)-Quantile geplottet. 
```{r}
qqnorm(lin_reg$residuals)
qqline(lin_reg$residuals)
```

Erklarungsgehalt fur die oberen Werte ist bisschen schwacher, deswegen passt das hier nicht genau. (Im oberen Bereich unterschatzen wir das Prestige tendenziell) 

```{r}
plot(lin_reg)
```


